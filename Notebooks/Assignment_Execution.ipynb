{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a87da32",
   "metadata": {},
   "source": [
    "# Clio Muse Tours Plan Execution\n",
    "\n",
    "## Project Structure & Execution\n",
    "0. [**Discovery**](#discovery): Identify the business frame we will work on\n",
    "1. [**Setup**](Assignment_Execution.ipynb#setup): Create folder structure and install dependencies\n",
    "3. [**Explore Data**](#explore-data): Explore how data are structured in each file.\n",
    "2. [**Cleaning**](#cleaning): Apply cleaning rules and validate processed data\n",
    "3. [**Transformation**](#transformation): Create processed tables where we merge different dataset to achieve different outcomes\n",
    "4. [**Analysis**](#analysis): Generate insights before clustering\n",
    "5. [**Clustering**](#clustering): Segment data to provide further structured insights\n",
    "6. [**Visualization**](#visualization): Create visualizations here or build PowerBI Dashboards\n",
    "7. [**Reporting**](#reporting): Synthesize findings in a presentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee970ab4",
   "metadata": {},
   "source": [
    "## Discovery\n",
    "\n",
    "### Business Questions\n",
    "\n",
    "* **Business Question**: Booking lead time analysis\n",
    "    * **Analytical Objective**: Quantify time between booking and travel\n",
    "    * **Success Metric**: Lead time distribution, median/mean\n",
    "* **Business Question**: Refund rates by product\n",
    "    * **Analytical Objective**: Identify high-risk products\n",
    "    * **Success Metric**: Refund rate % by product_type, product_country\n",
    "* **Business Question**: Group size vs refund behavior\n",
    "    * **Analytical Objective**: Understand group booking patterns\n",
    "    * **Success Metric**: Correlation coefficient, refund rate by group size\n",
    "* **Business Question**: Content consumption depth\n",
    "    * **Analytical Objective**: Measure engagement quality\n",
    "    * **Success Metric**: Completion rates, average listening duration\n",
    "* **Business Question**: Drop-off points\n",
    "    * **Analytical Objective**: Identify content friction\n",
    "    * **Success Metric**: Story-level abandonment rates\n",
    "* **Business Question**: User segemntation\n",
    "    * **Analytical Objective**: Create actionable personas\n",
    "    * **Success Metric**: 3-5 distinct clusters with interpretable behaviors\n",
    "---\n",
    "\n",
    "> Visulizations need to enable decision making\n",
    "\n",
    "### Constraints & Assumptions\n",
    "- **Time Range Mismatch**: Bookings (2024) vs App data (2025) - analyze separately, note limitation\n",
    "- **User Identity**: `user_id` (logged-in) vs `user_pseudo_id` (anonymous) - handle both\n",
    "- **Missing Data**: Some events lack user_id - use pseudo_id for anonymous analysis\n",
    "- **Timestamp Precision**: Microseconds - convert to datetime, handle timezone (assume UTC)\n",
    "- **Language Mapping**: `language` field (e.g., \"pt-pt\") vs `lang_id` - reconcile via mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21435ee",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b311e",
   "metadata": {},
   "source": [
    "Paste the following commands in the terminal and wait for them to be installed:\n",
    "```\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "pip install scikit-learn\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "pip install plotly\n",
    "pip install jupyter\n",
    "pip install ipykernel\n",
    "pip install pyarrow\n",
    "pip install openpyxl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e42a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce13d6",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b7bbf",
   "metadata": {},
   "source": [
    "Let's start by loading the data and watching them at a high level.\n",
    "\n",
    "For that we'll use glob to make our live easier and manually load the data for each month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2004b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\konst\\AppData\\Local\\Temp\\ipykernel_13140\\3220622363.py:7: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(path, **read_csv_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users files: ['../Data/raw/users\\\\users_data_2025-07.csv', '../Data/raw/users\\\\users_data_2025-08.csv', '../Data/raw/users\\\\users_data_2025-09.csv', '../Data/raw/users\\\\users_data_2025-10.csv']\n",
      "Users shape: (52743, 6)\n",
      "------------------------------\n",
      "Events files: ['../Data/raw/events\\\\events_data_2025-07.csv', '../Data/raw/events\\\\events_data_2025-08.csv', '../Data/raw/events\\\\events_data_2025-09.csv', '../Data/raw/events\\\\events_data_2025-10.csv']\n",
      "Events shape: (13471335, 13)\n",
      "------------------------------\n",
      "Bookings shape: (163828, 10)\n",
      "------------------------------\n",
      "ID Language Mappings shape: (22, 4)\n",
      "------------------------------\n",
      "Tour Title Mappings shape: (199, 2)\n"
     ]
    }
   ],
   "source": [
    "def load_many_csv(pattern, **read_csv_kwargs):\n",
    "    paths = sorted(glob.glob(pattern))\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path, **read_csv_kwargs)\n",
    "        df['source_file'] = path.split(\"/\")[-1]  # Add a column to identify the source file\n",
    "        dfs.append(df)\n",
    "\n",
    "    out = pd.concat(dfs, ignore_index=True)\n",
    "    return out, paths\n",
    "\n",
    "# load data: users, events, mappings, bookings\n",
    "users_all, user_paths = load_many_csv(\"../Data/raw/users/users_data_2025-*.csv\")\n",
    "events_all, event_paths = load_many_csv(\"../Data/raw/events/events_data_2025-*.csv\")\n",
    "bookings_data = pd.read_csv(\"../Data/raw/bookings/travellers_data_2024.csv\")\n",
    "id_lang_mappings_df = pd.read_csv(\"../Data/raw/mappings/id_language_mapping.csv\")\n",
    "tour_title_mapping_df = pd.read_csv(\"../Data/raw/mappings/tour_title_mapping.csv\")\n",
    "\n",
    "print(f\"Users files: {user_paths}\")\n",
    "print(f\"Users shape: {users_all.shape}\")\n",
    "print(\"---\" * 10)\n",
    "print(f\"Events files: {event_paths}\")\n",
    "print(f\"Events shape: {events_all.shape}\")\n",
    "print(\"---\" * 10)\n",
    "print(f\"Bookings shape: {bookings_data.shape}\")\n",
    "print(\"---\" * 10)\n",
    "print(f\"ID Language Mappings shape: {id_lang_mappings_df.shape}\")\n",
    "print(\"---\" * 10)\n",
    "print(f\"Tour Title Mappings shape: {tour_title_mapping_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c893f5",
   "metadata": {},
   "source": [
    "Okay, so we have:\n",
    "- **Users:** 52743 records of users \n",
    "    - **Questions for us (the team):**\n",
    "        - Are all the records corresponding to unique users?\n",
    "- **Events:** 13,471,335 events recorded\n",
    "    - **Questions for us (the team):**\n",
    "        - Are all the records corresponding to an existing user?\n",
    "- **Bookings:** 163828 bookigns for 2024\n",
    "- **Id Language Mapping:** 22 Different Language supported\n",
    "- **Tour Titles:** Currently supporting 199 tours inside the app.\n",
    "\n",
    "---\n",
    "\n",
    "Okay great, we have a certain high-level idea of our data now let's dig a little deeper.\n",
    "\n",
    "We'll check all columns per dataset to confirm:\n",
    "- `dtypes`\n",
    "- `missingness`\n",
    "- `uniqueness`\n",
    "- `ranges`\n",
    "\n",
    "For that we'll use a function that takes a dataframe, and returns for each dataset, a profiled dataframe of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f18a80f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      dataset                             column                dtype   rows  \\\n",
      "1  Users Data                first_purchase_date       datetime64[ns]  52743   \n",
      "0  Users Data                            user_id                int64  52743   \n",
      "2  Users Data  user_first_touch_timestamp_micros                int64  52743   \n",
      "6  Users Data                 first_touch_ts_utc  datetime64[ns, UTC]  52743   \n",
      "4  Users Data                            country               string  52743   \n",
      "5  Users Data                        source_file               object  52743   \n",
      "3  Users Data                   operating_system               string  52743   \n",
      "\n",
      "   missing  missing_pct  nunique           min           p50           p99  \\\n",
      "1    52216         0.99       96           NaN           NaN           NaN   \n",
      "0        0         0.00    27171  0.000000e+00  5.325760e+05  5.812077e+05   \n",
      "2        0         0.00     1043  1.666224e+15  1.752797e+15  1.761523e+15   \n",
      "6        0         0.00     1043           NaN           NaN           NaN   \n",
      "4        0         0.00      151           NaN           NaN           NaN   \n",
      "5        0         0.00        4           NaN           NaN           NaN   \n",
      "3        0         0.00        2           NaN           NaN           NaN   \n",
      "\n",
      "            max                         min_dt                         max_dt  \n",
      "1           NaN  1970-01-01 00:00:00.020230130  1970-01-01 00:00:00.020250827  \n",
      "0  5.831770e+05                            NaN                            NaN  \n",
      "2  1.761869e+15                            NaN                            NaN  \n",
      "6           NaN      2022-10-20 00:00:00+00:00      2025-10-31 00:00:00+00:00  \n",
      "4           NaN                            NaN                            NaN  \n",
      "5           NaN                            NaN                            NaN  \n",
      "3           NaN                            NaN                            NaN  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Profile the loaded DataFrames\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(profile_df(users_all, name=\u001b[33m\"\u001b[39m\u001b[33mUsers Data\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprofile_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvents Data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# print(profile_df(bookings_data, name=\"Bookings Data\"))\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# print(profile_df(id_lang_mappings_df, name=\"ID Language Mappings\"))\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# print(profile_df(tour_title_mapping_df, name=\"Tour Title Mappings\"))\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mprofile_df\u001b[39m\u001b[34m(df, name, top_n)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Numeric stats\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_numeric_dtype(s):\n\u001b[32m     22\u001b[39m     row.update({\n\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.nanmin(s)),\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mp50\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanpercentile\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m),\n\u001b[32m     25\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mp99\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.nanpercentile(s, \u001b[32m99\u001b[39m)),\n\u001b[32m     26\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(np.nanmax(s))\n\u001b[32m     27\u001b[39m     })\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Datetime stats\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pd.api.types.is_datetime64_any_dtype(s):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1406\u001b[39m, in \u001b[36mnanpercentile\u001b[39m\u001b[34m(a, q, axis, out, overwrite_input, method, keepdims, weights, interpolation)\u001b[39m\n\u001b[32m   1403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.any(weights < \u001b[32m0\u001b[39m):\n\u001b[32m   1404\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWeights must be non-negative.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nanquantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1617\u001b[39m, in \u001b[36m_nanquantile_unchecked\u001b[39m\u001b[34m(a, q, axis, out, overwrite_input, method, keepdims, weights)\u001b[39m\n\u001b[32m   1615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m a.size == \u001b[32m0\u001b[39m:\n\u001b[32m   1616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.nanmean(a, axis, out=out, keepdims=keepdims)\n\u001b[32m-> \u001b[39m\u001b[32m1617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1618\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_nanquantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1619\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1620\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1623\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1624\u001b[39m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1625\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_function_base_impl.py:3914\u001b[39m, in \u001b[36m_ureduce\u001b[39m\u001b[34m(a, func, keepdims, **kwargs)\u001b[39m\n\u001b[32m   3911\u001b[39m     index_out = (\u001b[32m0\u001b[39m, ) * nd\n\u001b[32m   3912\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m] = out[(\u001b[38;5;28mEllipsis\u001b[39m, ) + index_out]\n\u001b[32m-> \u001b[39m\u001b[32m3914\u001b[39m r = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1645\u001b[39m, in \u001b[36m_nanquantile_ureduce_func\u001b[39m\u001b[34m(a, q, weights, axis, out, overwrite_input, method)\u001b[39m\n\u001b[32m   1643\u001b[39m     part = a.ravel()\n\u001b[32m   1644\u001b[39m     wgt = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m weights.ravel()\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m     result = \u001b[43m_nanquantile_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1646\u001b[39m \u001b[38;5;66;03m# Note that this code could try to fill in `out` right away\u001b[39;00m\n\u001b[32m   1647\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1696\u001b[39m, in \u001b[36m_nanquantile_1d\u001b[39m\u001b[34m(arr1d, q, overwrite_input, method, weights)\u001b[39m\n\u001b[32m   1692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr1d.size == \u001b[32m0\u001b[39m:\n\u001b[32m   1693\u001b[39m     \u001b[38;5;66;03m# convert to scalar\u001b[39;00m\n\u001b[32m   1694\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.full(q.shape, np.nan, dtype=arr1d.dtype)[()]\n\u001b[32m-> \u001b[39m\u001b[32m1696\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfnb\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_quantile_unchecked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m    \u001b[49m\u001b[43marr1d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_function_base_impl.py:4569\u001b[39m, in \u001b[36m_quantile_unchecked\u001b[39m\u001b[34m(a, q, axis, out, overwrite_input, method, keepdims, weights)\u001b[39m\n\u001b[32m   4560\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_quantile_unchecked\u001b[39m(a,\n\u001b[32m   4561\u001b[39m                         q,\n\u001b[32m   4562\u001b[39m                         axis=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4566\u001b[39m                         keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   4567\u001b[39m                         weights=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   4568\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4569\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4570\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_quantile_ureduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4571\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4572\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4573\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4574\u001b[39m \u001b[43m                    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4575\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4576\u001b[39m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4577\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_function_base_impl.py:3914\u001b[39m, in \u001b[36m_ureduce\u001b[39m\u001b[34m(a, func, keepdims, **kwargs)\u001b[39m\n\u001b[32m   3911\u001b[39m     index_out = (\u001b[32m0\u001b[39m, ) * nd\n\u001b[32m   3912\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m] = out[(\u001b[38;5;28mEllipsis\u001b[39m, ) + index_out]\n\u001b[32m-> \u001b[39m\u001b[32m3914\u001b[39m r = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_function_base_impl.py:4744\u001b[39m, in \u001b[36m_quantile_ureduce_func\u001b[39m\u001b[34m(a, q, weights, axis, out, overwrite_input, method)\u001b[39m\n\u001b[32m   4742\u001b[39m     arr = a.copy()\n\u001b[32m   4743\u001b[39m     wgt = weights\n\u001b[32m-> \u001b[39m\u001b[32m4744\u001b[39m result = \u001b[43m_quantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4745\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4746\u001b[39m \u001b[43m                   \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4747\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4748\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4749\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4750\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_function_base_impl.py:4876\u001b[39m, in \u001b[36m_quantile\u001b[39m\u001b[34m(arr, quantiles, axis, method, out, weights)\u001b[39m\n\u001b[32m   4874\u001b[39m         result_shape = virtual_indexes.shape + (\u001b[32m1\u001b[39m,) * (arr.ndim - \u001b[32m1\u001b[39m)\n\u001b[32m   4875\u001b[39m         gamma = gamma.reshape(result_shape)\n\u001b[32m-> \u001b[39m\u001b[32m4876\u001b[39m         result = \u001b[43m_lerp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprevious\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4877\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4878\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4879\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4881\u001b[39m     \u001b[38;5;66;03m# Weighted case\u001b[39;00m\n\u001b[32m   4882\u001b[39m     \u001b[38;5;66;03m# This implements method=\"inverted_cdf\", the only supported weighted\u001b[39;00m\n\u001b[32m   4883\u001b[39m     \u001b[38;5;66;03m# method, which needs to sort anyway.\u001b[39;00m\n\u001b[32m   4884\u001b[39m     weights = np.asanyarray(weights)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\numpy\\lib\\_function_base_impl.py:4671\u001b[39m, in \u001b[36m_lerp\u001b[39m\u001b[34m(a, b, t, out)\u001b[39m\n\u001b[32m   4657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_lerp\u001b[39m(a, b, t, out=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   4658\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4659\u001b[39m \u001b[33;03m    Compute the linear interpolation weighted by gamma on each point of\u001b[39;00m\n\u001b[32m   4660\u001b[39m \u001b[33;03m    two same shape array.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4669\u001b[39m \u001b[33;03m        Output array.\u001b[39;00m\n\u001b[32m   4670\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4671\u001b[39m     diff_b_a = \u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4672\u001b[39m     \u001b[38;5;66;03m# asanyarray is a stop-gap until gh-13105\u001b[39;00m\n\u001b[32m   4673\u001b[39m     lerp_interpolation = asanyarray(add(a, diff_b_a * t, out=out))\n",
      "\u001b[31mTypeError\u001b[39m: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead."
     ]
    }
   ],
   "source": [
    "def profile_df(df, name=\"df\", top_n=5):\n",
    "    prof = []\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        dtype = str(s.dtype)\n",
    "        missing = int(s.isna().sum())\n",
    "        missing_pct = float(missing / len(df)) if len(df) else 0.0\n",
    "        nunique = int(s.nunique(dropna=True))\n",
    "\n",
    "        row = {\n",
    "            \"dataset\": name,\n",
    "            \"column\": col,\n",
    "            \"dtype\": dtype,\n",
    "            \"rows\": len(df),\n",
    "            \"missing\": missing,\n",
    "            \"missing_pct\": round(missing_pct, 4),\n",
    "            \"nunique\": nunique\n",
    "        }\n",
    "\n",
    "        # Numeric stats\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            row.update({\n",
    "                \"min\": float(np.nanmin(s)),\n",
    "                \"p50\": float(np.nanpercentile(s, 50)),\n",
    "                \"p99\": float(np.nanpercentile(s, 99)),\n",
    "                \"max\": float(np.nanmax(s))\n",
    "            })\n",
    "\n",
    "        # Datetime stats\n",
    "        if pd.api.types.is_datetime64_any_dtype(s):\n",
    "            row.update({\n",
    "                \"min_dt\": str(s.min()),\n",
    "                \"max_dt\": str(s.max())\n",
    "            })\n",
    "\n",
    "        prof.append(row)\n",
    "\n",
    "    prof_df = pd.DataFrame(prof).sort_values([\"missing_pct\",\"nunique\"], ascending=[False, False])\n",
    "    return prof_df\n",
    "\n",
    "# Profile the loaded DataFrames\n",
    "print(profile_df(users_all, name=\"Users Data\"))\n",
    "print(profile_df(events_all, name=\"Events Data\"))\n",
    "# print(profile_df(bookings_data, name=\"Bookings Data\"))\n",
    "# print(profile_df(id_lang_mappings_df, name=\"ID Language Mappings\"))\n",
    "# print(profile_df(tour_title_mapping_df, name=\"Tour Title Mappings\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38ee5b8",
   "metadata": {},
   "source": [
    "How we can intepret results here:\n",
    "**Users Data – Key Findings**\n",
    "- `first_purchase_date` is almost entirely missing (≈99%)\n",
    "    - Any purchase-related analysis using this field must, clearly state that purchase data coverage is extremely limited\n",
    "\n",
    "- `user_id` is complete and highly unique\n",
    "    - 52,743 rows with 0% missing\n",
    "    - ~27k unique user IDs → users appear multiple times across months\n",
    "    - Confirms the need for deduplication into a dim_user table\n",
    "\n",
    "- `user_first_touch_timestamp_micros` is complete\n",
    "    - 0% missing which means we have a strong foundation for time-to-first-action metrics\n",
    "    - Microsecond values confirm high-precision timestamps that must be converted to UTC datetime\n",
    "\n",
    "- Country and OS are fully populated\n",
    "    - `country`: 151 unique values, wide geographic spread\n",
    "    - `operating_system`: only 2 values, iOS vs Android\n",
    "    - Enables strong country-level and OS-level segmentation\n",
    "\n",
    "**Events Data – Key Findings**\n",
    "\n",
    "- Audio interaction fields are mostly missing\n",
    "    - `audio_time_played`: ~99.6% missing\n",
    "    - `audio_time_paused`: ~99.3% missing\n",
    "    - You cannot assume every event represents listening behavior\n",
    "\n",
    "- User identity is split between logged-in and anonymous\n",
    "    - `user_id` missing in ~41% of events\n",
    "    - `user_pseudo_id` has 0% missing\n",
    "    - So what we need to is:\n",
    "        - Treat anonymous and logged-in users separately\n",
    "        - Full enrichment (country, OS) is only possible for logged-in users\n",
    "\n",
    "- Event timestamps are complete and precise\n",
    "    - `event_timestamp` has 0% missing and ~13.47M unique values\n",
    "\n",
    "- Event date is numeric, not datetime\n",
    "    - `event_date` is stored as an integer (YYYYMMDD)\n",
    "    - Must be converted and validated against event_timestamp\n",
    "\n",
    "\n",
    "- Tour and story coverage is broad but incomplete\n",
    "    - `tour_id`: ~5% missing → some events are not tied to tours\n",
    "    - `story_id`: ~20% missing → not all events are story-level\n",
    "    - Analysis must:\n",
    "        - filter to tour/story-related events when measuring listening depth\n",
    "        - avoid assuming all events represent content consumption\n",
    "\n",
    "- Language complexity exists\n",
    "    - `language` has 416 unique values (e.g. pt-pt, en-us)\n",
    "    - `lang_id` only has 9 unique values\n",
    "    - Confirms need for:\n",
    "        - normalization of app language\n",
    "        - reconciliation with lang_id via mapping\n",
    "\n",
    "---\n",
    "\n",
    "Okay now that we know our data very well, and understand  let's return to confirm our assumptions now that we know a lot more about our data\n",
    "\n",
    "1. Time Range Mismatch (2024 bookings vs 2025 events/users)\n",
    "    - For that we'll first standardize timestamps from milliseconds, to actual datetimes\n",
    "    - For `users_all`, we'll standardize `first_purchase_date` by transforming it into datetime if available and `user_first_touch_timestamp_micros` where we are adding a new column with standardize datetime\n",
    "    - The same process goes on for events and bookings too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32de9f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users standardized dtypes:\n",
      "user_id                                            int64\n",
      "first_purchase_date                       datetime64[ns]\n",
      "user_first_touch_timestamp_micros                  int64\n",
      "operating_system                          string[python]\n",
      "country                                   string[python]\n",
      "source_file                                       object\n",
      "first_touch_ts_utc                   datetime64[ns, UTC]\n",
      "dtype: object\n",
      "Bookings standardized dtypes:\n",
      "booking_id                    int64\n",
      "travel_date          datetime64[ns]\n",
      "travel_slot          string[python]\n",
      "booking_date         datetime64[ns]\n",
      "product_code         string[python]\n",
      "product_title        string[python]\n",
      "product_type         string[python]\n",
      "product_country      string[python]\n",
      "num_of_travellers             int64\n",
      "booking_status       string[python]\n",
      "dtype: object\n",
      "Events standardized dtypes:\n",
      "event_date                datetime64[ns]\n",
      "event_timestamp                    int64\n",
      "event_name                string[python]\n",
      "platform                  string[python]\n",
      "language                          object\n",
      "user_id                          float64\n",
      "user_pseudo_id                    object\n",
      "tour_id                          float64\n",
      "story_id                         float64\n",
      "lang_id                          float64\n",
      "audio_time_played                 object\n",
      "audio_time_paused                 object\n",
      "source_file                       object\n",
      "event_ts_utc         datetime64[ns, UTC]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def standardize_users(users_df):\n",
    "    df = users_df.copy()\n",
    "\n",
    "    # Dates\n",
    "    df[\"first_purchase_date\"] = pd.to_datetime(df[\"first_purchase_date\"], errors=\"coerce\")\n",
    "\n",
    "    # Micros → datetime UTC\n",
    "    df[\"first_touch_ts_utc\"] = pd.to_datetime(\n",
    "        df[\"user_first_touch_timestamp_micros\"],\n",
    "        unit=\"us\",\n",
    "        utc=True,\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Clean categories\n",
    "    df[\"operating_system\"] = df[\"operating_system\"].astype(\"string\").str.strip()\n",
    "    df[\"country\"] = df[\"country\"].astype(\"string\").str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "users_all = standardize_users(users_all)\n",
    "print(\"Users standardized dtypes:\")\n",
    "print(users_all.dtypes)\n",
    "\n",
    "def standardize_bookings(bookings_df):\n",
    "    df = bookings_df.copy()\n",
    "\n",
    "    df[\"booking_date\"] = pd.to_datetime(df[\"booking_date\"], errors=\"coerce\")\n",
    "    df[\"travel_date\"] = pd.to_datetime(df[\"travel_date\"], errors=\"coerce\")\n",
    "\n",
    "    # Group size\n",
    "    df[\"num_of_travellers\"] = pd.to_numeric(df[\"num_of_travellers\"], errors=\"coerce\")\n",
    "\n",
    "    # Basic cleanup\n",
    "    for c in [\"product_code\",\"product_title\",\"product_type\",\"product_country\",\"booking_status\",\"travel_slot\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\").str.strip()\n",
    "\n",
    "    return df\n",
    "\n",
    "bookings = standardize_bookings(bookings_data)\n",
    "print(\"Bookings standardized dtypes:\")\n",
    "print(bookings.dtypes)  \n",
    "\n",
    "def standardize_events(events_df):\n",
    "    df = events_df.copy()\n",
    "\n",
    "    df[\"event_date\"] = pd.to_datetime(df[\"event_date\"], errors=\"coerce\")\n",
    "\n",
    "    # Micros → datetime UTC\n",
    "    df[\"event_ts_utc\"] = pd.to_datetime(\n",
    "        df[\"event_timestamp\"],\n",
    "        unit=\"us\",\n",
    "        utc=True,\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    # Clean categories\n",
    "    df[\"event_name\"] = df[\"event_name\"].astype(\"string\").str.strip()\n",
    "    df[\"platform\"] = df[\"platform\"].astype(\"string\").str.strip()\n",
    "\n",
    "    return df\n",
    "events_all = standardize_events(events_all)\n",
    "print(\"Events standardized dtypes:\")\n",
    "print(events_all.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a7bf84",
   "metadata": {},
   "source": [
    "The users dataset is now cleanly typed and analysis-ready, with correct datetime parsing and categorical normalization.\n",
    "\n",
    "The transformation confirms that temporal user behavior analysis is feasible, while monetization analysis is limited due to sparse purchase data.\n",
    "\n",
    "---\n",
    "\n",
    "Okay now the timestamps are ready standardized into datetime for all necessary dfs:\n",
    "- events_all\n",
    "- users_all\n",
    "- bookings_data\n",
    "\n",
    "Now let's just use min/max to prove what we were thinking about our time range mismatch, and see if we can in any way connect bookings with users/events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f8c66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bookings date range: 2023-01-31 → 2025-04-10 | Travel: 2024-01-01 → 2024-12-31\n",
      "Users first_touch range: 2022-10-20 00:00:00+00:00 → 2025-10-31 00:00:00+00:00\n",
      "Events event_date range: 2025-06-30 21:00:33.442000+00:00 → 2025-10-31 21:58:37.367000+00:00\n"
     ]
    }
   ],
   "source": [
    "print(\"Bookings date range:\",\n",
    "      bookings_data[\"booking_date\"].min(), \"→\", bookings_data[\"booking_date\"].max(),\n",
    "      \"| Travel:\", bookings_data[\"travel_date\"].min(), \"→\", bookings_data[\"travel_date\"].max())\n",
    "\n",
    "print(\"Users first_touch range:\",\n",
    "      users_all[\"first_touch_ts_utc\"].min(), \"→\", users_all[\"first_touch_ts_utc\"].max())\n",
    "\n",
    "print(\"Events event_date range:\",\n",
    "      events_all[\"event_ts_utc\"].min(), \"→\", events_all[\"event_ts_utc\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abb0cd",
   "metadata": {},
   "source": [
    "Okay how do we interpet these data:\n",
    "- The bookings dataset represents a complete travel year (2024), with bookings occurring both before and after the travel period, enabling robust lead-time and seasonality analysis.\n",
    "\n",
    "- The app events dataset captures user engagement during July–October 2025, and is analyzed independently from bookings due to non-overlapping timeframes.\n",
    "\n",
    "- User onboarding data spans multiple years and provides historical context for engagement patterns observed in 2025.\n",
    "\n",
    "---\n",
    "\n",
    "2. **User Identity**: `user_id` (logged-in) vs `user_pseudo_id` (anonymous) - handle both\n",
    "- for that we have a function to identify how many rows have only `user_pseudo_id` and how many have both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f25b7016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "identity_type\n",
       "both              7945646\n",
       "anonymous_only    5525689\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identity_breakdown(events_df):\n",
    "    df = events_df.copy()\n",
    "\n",
    "    has_user = df[\"user_id\"].notna()\n",
    "    has_pseudo = df[\"user_pseudo_id\"].notna()\n",
    "\n",
    "    df[\"identity_type\"] = np.select(\n",
    "        [has_user & has_pseudo, has_user & ~has_pseudo, ~has_user & has_pseudo],\n",
    "        [\"both\", \"logged_in_only\", \"anonymous_only\"],\n",
    "        default=\"no_id\"\n",
    "    )\n",
    "\n",
    "    return df[\"identity_type\"].value_counts(dropna=False)\n",
    "\n",
    "# Example usage after you load events_all:\n",
    "identity_breakdown(events_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addef3c6",
   "metadata": {},
   "source": [
    "Okay how do we interpet these data:\n",
    "\n",
    "- The events dataset contains two dominant identity states:\n",
    "    - events where both user_id and user_pseudo_id are present\n",
    "    - events where only user_pseudo_id is present\n",
    "- There are no events with only user_id and no events with missing identity entirely.\n",
    "\n",
    "- `user_pseudo_id` is the primary tracking backbone\n",
    "    - Present in 100% of events\n",
    "    - Enables:\n",
    "        - sessionization for all users\n",
    "        - anonymous engagement analysis\n",
    "        - pre-login behavior tracking\n",
    "- `user_id` acts as an enrichment layer\n",
    "    - Appears only after authentication\n",
    "    - Adds:\n",
    "        - geographic context\n",
    "        - OS metadata\n",
    "        - user lifecycle timestamps\n",
    "    - Its absence does not block behavioral analysis, only enrichment\n",
    "- Logged-in users still retain their pseudo identity\n",
    "    - Confirms that:\n",
    "        - authentication does not reset device tracking\n",
    "        - behavior before and after login can be stitched via pseudo_id\n",
    "    - This opens the door to:\n",
    "        - login impact analysis\n",
    "        - transition analysis (anonymous → logged-in)\n",
    "---\n",
    "\n",
    "3. **Missing Data**: Some events lack user_id - use pseudo_id for anonymous analysis\n",
    "    - For that we'll have a function which takes our major key columns, and output the percentage of events that don't have the corresponding column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "510cb809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id            41.02\n",
       "story_id           20.41\n",
       "lang_id             5.95\n",
       "tour_id             4.70\n",
       "event_name          0.00\n",
       "event_timestamp     0.00\n",
       "event_date          0.00\n",
       "user_pseudo_id      0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def missing_key_fields(events_df):\n",
    "    keys = [\"event_date\",\"event_timestamp\",\"event_name\",\"tour_id\",\"story_id\",\"user_id\",\"user_pseudo_id\",\"lang_id\"]\n",
    "    existing = [k for k in keys if k in events_df.columns]\n",
    "    return (events_df[existing].isna().mean().sort_values(ascending=False) * 100).round(2)\n",
    "\n",
    "missing_key_fields(events_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698f3cf8",
   "metadata": {},
   "source": [
    "Okay how do we interpet these data:\n",
    "\n",
    "- Missing data in the events dataset is structured and expected, not random.\n",
    "- Core tracking fields are fully populated, while content- and identity-related fields are conditionally missing depending on event type and user state.\n",
    "- This validates the hypothesis that user_pseudo_id must be used as the backbone identifier, with user_id used only when available.\n",
    "\n",
    "- `user_id` - 41.02% events don't have `user_id`\n",
    "    - Any analysis requiring demographics must be restricted to the 59% logged in subset\n",
    "    - We don't need to drop these rows but instead we can rely on `user_pseudo_id`\n",
    "\n",
    "- `story_id` - 20.41% events doesn't have a story_id\n",
    "    - Many events are:\n",
    "        - navigation events\n",
    "        - app lifecycle events\n",
    "        - non-audio interactions\n",
    "    - **The problem**: we can't treat all events as story-level interactions.\n",
    "\n",
    "- `lang_id` - 5.95% events don't have language info\n",
    "    - analysis of language should stick to non-null `lang_id`\n",
    "\n",
    "- `tour_id` - 4.70% events don't have a tour \n",
    "    - Tour level metrics must\n",
    "        - filter to non-null `tour_id`\n",
    "        - avoid assuming all events relate to content consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6567d7c",
   "metadata": {},
   "source": [
    "### How do we proceed for cleaning?\n",
    "\n",
    "1. For events:\n",
    "    - We only need `event_ts_utc` as our only authoritative time field\n",
    "    - Not all events are listening behavior\n",
    "2. For users:\n",
    "    - `user_pseudo_id` is our primary behavioral key\n",
    "    - `user_id` is used only for enrichment if present\n",
    "    - dedup users across monthly files by grabbing the user's first touch timestamp, so that we will have one row per user\n",
    "3. Remove null ids of `tour_id` `story_id` `lang_id` and turn the present ids to integers.\n",
    "4. Clean Bookings Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a30371",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "After data clean we need to end up with the following tables\n",
    "- `dim_users`: one row per user, enriched, deduplicated\n",
    "- `events_clean`: all events, cleaned timestamps, identity logic applied\n",
    "- `listening_events`: subset of events eligible for audio analysis\n",
    "- `bookings_clean`: booking with lead time, cleaned categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a8c8f",
   "metadata": {},
   "source": [
    "## Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25cfb90",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4b0bf",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50fe54",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74542797",
   "metadata": {},
   "source": [
    "## Reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
